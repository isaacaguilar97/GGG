ggplot(data = bikes) +
geom_histogram(mapping = aes(x = cnt, y = ..density..),
binwidth = 100) +
theme_bw() +
theme(aspect.ratio = 1)
bikes_model <- bestglm(as.data.frame(bikes),
IC = "BIC",
method = "exhaustive",
TopModels = 1,
family = poisson)
summary(bikes_model$BestModel)
bikes_poisson <- glm(cnt ~ season + yr + holiday +
workingday + weathersit + temp + hum + windspeed,
data = bikes,
family = poisson(link = "log"))
summary(bikes_poisson)
#Temperature
ggplot(data = bikes, mapping = aes(y = log(cnt + 1), x = temp)) +
geom_point() +
theme_bw() +
theme(aspect.ratio = 1)
#Humidity
ggplot(data = bikes, mapping = aes(y = log(cnt + 1), x = hum)) +
geom_point() +
theme_bw() +
theme(aspect.ratio = 1)
#Windspeed
ggplot(data = bikes, mapping = aes(y = log(cnt + 1), x = windspeed)) +
geom_point() +
theme_bw() +
theme(aspect.ratio = 1)
# Use added variable plots for any continuous predictors you included in the
# model
avPlots(bikes_poisson, terms = ~ temp + hum + windspeed)
bikes.cooks <- data.frame("cooks.distance" = cooks.distance(bikes_poisson))
bikes.cooks$obs <- 1:nrow(bikes)
ggplot(data = bikes.cooks) +
geom_point(mapping = aes(x = obs, y = abs(cooks.distance))) +
geom_hline(mapping = aes(yintercept = 4/ length(obs)),
color = "red", linetype = "dashed") +  # for n > 30
geom_hline(mapping = aes(yintercept = 1),
color = "red", linetype = "dashed") +  # for n > 30
theme_bw() +
theme(aspect.ratio = 1)
bikes$cooksd <- cooks.distance(bikes_poisson)
bikes %>%
mutate(rowNum = row.names(bikes)) %>%  # save original row numbers
filter(cooksd > 4 / length(cooksd)) %>%  # select potential outliers
arrange(desc(cooksd))
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
FatComplete <- read_table("BodyFat.txt")
bodyfat<- FatComplete %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19)
round(cor(bodyfat), 2)
corrplot(cor(bodyfat), type = "upper")
bodyfat_lm <- lm(brozek ~ ., data = bodyfat)
summary(bodyfat_lm)
bodyfat$residuals <- bodyfat_lm$residuals
bodyfat_resid_vs_fit <- autoplot(bodyfat_lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
bodyfat_resid_vs_fit
plot4
## Loading Libraries
library(tidyverse)
library(vroom)
library(DataExplorer)
library(patchwork)
# Load data
bike <- vroom('./train.csv')
#install.packages("sentimentr")
library(sentimentr)
#SET WORKING DIRECTORY
setwd("C:/Users/georgem/Box/REDA - Research, Evaluation & Data Analytics/R - Text Analysis/Sentiment Analysis")
getwd()
setwd('/Users/isaacaguilar/Library/CloudStorage/Box-Box/REDA - Research, Evaluation & Data Analytics/Isaac/Sentimentr')
some_text <- c('This is a very beautiful day',
'I am not feeling good. I hate it when it happens',
'the product is good but expensive at this price',
'I love you')
sentiment(some_text) #GIVES SENTIMENT BY SENTENCE
library(tm)
#file.choose() creates popup for you to choose file
ISHS_Comments <- read.csv(file.choose(),header=TRUE)
#Examine Structure
str(ISHS_Comments)
#Build corpus
corpus <- iconv(ISHS_Comments,to="")
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus,tolower)
corpus <- tm_map(corpus,removePunctuation)
#corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
#create function to remove URLs
removeURL <- function(x) gsub('http[[:alnum:]]*', '',x)
#remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
#Term Document Matrix
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(tdm)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
comments2 <-iconv(ISHS_Comments$text, to="")
sentiment <- get_nrc_sentiment(comments2)
sentiment <- get_nrc_sentiment(comments2)
head(sentiment)
# Bar plot
barplot(colSums(sentiment),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores for ISHS Comments',
ylim = c(0,1200))
complaints
?write.csv
# library(syuzhet)
library(lubridate)
# library(scales)
library(reshape2)
# library(wordcloud)
# library(RColorBrewer)
# library(writexl)
# library(sentimentr)
# library(readxl)
library(tidyverse)
library(DataExplorer)
library(tm)
library(topicmodels)
setwd("/Users/isaacaguilar/Library/CloudStorage/Box-Box/REDA - Research, Evaluation & Data Analytics/Isaac")
data1 <- read.csv("EYES1num.csv")
data2 <- read.csv("EYES1txt.csv")
data3 <- read.csv("EYES2num.csv")
data4 <- read.csv("EYES2txt.csv")
# keep only the columns from the first dataframe and drop any columns with the
# same name from the subsequent dataframes during the merge
data <- data1 %>%
left_join(data2, by = "ResponseId", suffix = c("", ".y")) %>%
left_join(data3, by = "ResponseId", suffix = c("", ".y")) %>%
left_join(data4, by = "ResponseId", suffix = c("", ".y")) %>%
select(-matches("\\.y$"), -"Q_URL") %>%
select("Section", "Vers", everything())
complete_q <- data[1,] # Store complete questions in case we need
new_data <- data[-c(1,2),] # Remove the title that repeats
# Create Program Dictionary
program_dict <- list(
Q1 = "NOT_THIS",
Q2 = "ISUNIV",
Q3 = "ISHS",
Q4 = "wOMENCONF",
Q5 = "EFY",
Q6 = "CEW",
Q7 = "FAMHISTGENE",
Q8 = "FINPLAN",
Q9 = "COUNSELING",
Q10 = "ORGAN",
Q11 = "INTERMUSE",
Q12 = "RELFREEDOM",
Q13 = "PURPOSEFULPARENT",
Q14 = "FREECOURSES",
Q15 = "PURPOSEFULPARENT"
)
# Divide from general to program specific questions
general_q <- colnames(new_data)[!(grepl("^Q", colnames(new_data))) |
grepl("Q1\\.", colnames(new_data))]
numbered_q <- colnames(new_data)[grepl("^Q", colnames(new_data)) &
!(grepl("Q1\\.", colnames(new_data)))] # Select columns that  start with a Q
# Create a  copy of "vers" column called program
new_data$program <-  new_data$Vers
# Function
fill_missing_programs <- function(program_col, numbered_q, new_data, dict) {
results <- character(length(program_col))
for (i in 1:length(program_col)) {
program <- program_col[i]
if (program == "" || program == "[[version]]") {
# Check if any of the missing can go in an existing category or a new one
# ISHS -->  "AP " | "Grade | High School"
if (grepl("AP |Grade|High School", new_data[i, "Section"])){
results[i] <- "ISHS"
}
# ORGAN --> "Organ"
else if (grepl("Organ", new_data[i, "Section"])){
results[i] <- "ORGAN"
}
# TESTPREP --> "Test Prep"
else if (grepl("Test Prep", new_data[i, "Section"])){
results[i] <- "TESTPREP"
}
# ARC --> "ARC | Stake"
else if (grepl("ARC|Stake", new_data[i, "Section"])){
results[i] <- "ARC"
}
# COUNSELING --> "Counseling"
else if (grepl("Counseling", new_data[i, "Section"])){
results[i] <- "COUNSELING"
}
# EFY  --> "EFY"
else if (grepl("EFY", new_data[i, "Section"])){
results[i] <- "EFY"
}
# BALLET --> "Ballet"
# MERITBADGE --> "Merit Badge PowWow"
# SUMMERHONORS --> "Late Summer Honors"
# HORNSYMPO --> "Utah Horn Symposium"
# LDSDENTISTS --> "LDS Dentists"
#
else{
# Loop through columns that start with a Q
cln <- ""
for (col_name in numbered_q) {
if (!is.na(new_data[i, col_name]) && new_data[i, col_name] != "") {
# Store the column name and break the loop
cln <- sub("\\..*", "", col_name)
break
}
}
# Check if cln is empty, and provide a default value if needed
if (cln == "" | cln == "Q16") {
results[i] <- "OTHER"  # Replace with your default value
}
else {
print(cln)
results[i] <- dict[[cln]]
}
}
}
else {
results[i] <- program
}
}
return(results)
}
# Apply the function to the 'program' column in 'new_data'
new_data$program <- fill_missing_programs(new_data$program, numbered_q, new_data, program_dict)
# Check unique sections from Programs with value of "Other"
other_sections <- new_data %>%
filter(program == "OTHER") %>%
group_by(Section) %>%
summarise(Observations = n()) # Count number of rows
## Remove "OTHER" program category ####
# Data with Other Programs
data_o <- new_data %>%
filter(program == "OTHER")
# Data with main Programs
data_no <- new_data %>%
filter(program != "OTHER")
## Remove rows with missing satisfaction results ####
nrow(data_no %>% filter(Q1.4 == "")) / nrow(data_no) # Proportion of people that didn't answer this question
# proportion is low enough that we can get rid of those observations
data_no <- data_no %>% filter(Q1.4 != "")
## Turn duration to numeric
data_no$Duration..in.seconds. <- as.numeric(data_no$Duration..in.seconds.)
colnames(data_no)[8] <- "Duration_sec"
## I wanted to remove rows with duration bigger than 3 standard deviations
# But since they finished the survey and their answers seem find, I'll keep them
boxplot(data_no$Duration_sec)
# Z-score or Standard score
z_scores <- (data_no$Duration_sec - mean(data_no$Duration_sec)) / sd(data_no$Duration_sec)
sort(data_no$Duration_sec[abs(z_scores) > 2])
data_no$Duration_sec <- ifelse(abs(z_scores) > 2, mean(data_no$Duration_sec) + 2 * sd(data_no$Duration_sec) * sign(z_scores), data_no$Duration_sec)
data_no$Date <- as.Date(data_no$EndDate)
## Add Satisfaction Variable as integer ####
data_no$Satisfaction <- as.integer(data_no$Q1.4)
## Extract the year from the Date column ####
data_no <- data_no %>%
mutate(Year = format(Date, "%Y"))
data_no$Q1.16_1 <- ifelse(data_no$Q1.16_1 == "1", 1, 0)
data_no$Q1.16_2 <- ifelse(data_no$Q1.16_2 == "1", 1, 0)
data_no$Q1.16_3 <- ifelse(data_no$Q1.16_3 == "1", 1, 0)
data_no$Q1.16_4 <- ifelse(data_no$Q1.16_4 == "1", 1, 0)
data_no$Q1.17_1 <- ifelse(data_no$Q1.17_1 == "1", 1, 0)
data_no$Q1.17_2 <- ifelse(data_no$Q1.17_2 == "1", 1, 0)
data_no$Q1.17_3 <- ifelse(data_no$Q1.17_3 == "1", 1, 0)
data_no$Q1.17_4 <- ifelse(data_no$Q1.17_4 == "1", 1, 0)
### Progress column
# Convert Progress to numeric if it's not already
data_no$Progress <- as.numeric(data_no$Progress)
data_no %>%
group_by(Progress) %>%
summarize(Frequency = n())
data_no %>%
group_by(Finished) %>%
summarize(Frequency = n())
## Create new progress column with 3 categories Incomplete, Mostly done, Finished
data_no <- data_no %>%
mutate(progress_c = case_when(
Progress < 50 ~ "Incomplete",
Progress < 95 ~ "Mostly Done",
TRUE ~ "Finished"
))
data_no %>%
group_by(progress_c) %>%
summarise(frec = n())
# Use data from those that complete most of the survey
data_no_c <- data_no%>%
filter(progress_c == 'Finished') # Includes progress >= 95%
data_no_ic <- data_no%>%
filter(progress_c != 'Finished') # Includes progress < 95%
# Do it only for ISHS (Esier to get topics)
s_reasons <- data_no_c %>% filter(Q1.5 != "", program == "ISHS") %>% select(Q1.5) %>% rename(text = Q1.5)
expect <- data_no_c %>% filter(Q1.8 != "", program == "ISHS") %>% select(Q1.8) %>% rename(text = Q1.8)
quality <- data_no_c %>% filter(Q1.10 != "", program == "ISHS") %>% select(Q1.10) %>% rename(text = Q1.10)
money_compl <- data_no_c %>% filter(Q1.12 != "", program == "ISHS") %>% select(Q1.12) %>% rename(text = Q1.12)
concer_resp <- data_no_c %>% filter(Q1.14 != "", program == "ISHS") %>% select(Q1.14) %>% rename(text = Q1.14)
gen_feed <- data_no_c %>% filter(Q16.1 != "", program == "ISHS") %>% select(Q16.1) %>% rename(text = Q16.1)
# Put all complaints together
complaints <- rbind(s_reasons,expect,quality, money_compl,concer_resp, gen_feed)
write.csv(complaints, "ISHS_Comments.csv", row.names=False)
write.csv(complaints, "ISHS_Comments.csv", row.names=FALSE)
library(doParallel)
num_cores <- parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
library(tidyverse)
library(embed) # for target encoding
library(vroom)
library(DataExplorer)
library(patchwork)
library(tidymodels)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
setwd('~/College/Stat348/AmazonEmployeeAccess')
# Load data
amazon_train <- vroom('./train.csv')
amazon_train$ACTION <- as.factor(amazon_train$ACTION)
amazon_test <- vroom('./test.csv')
# Recipe
my_recipe <- recipe(ACTION~., data=amazon_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
# step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
# step_normalize(all_numeric_predictors()) %>%
# step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
step_smote(all_outcomes(), neighbors=5)
# Model
rf_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=250) %>%
set_engine("ranger") %>%
set_mode("classification")
## Workflow
amazon_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod) %>%
fit(data = amazon_train)
## Set up grid of tuning values
tuning_grid <- grid_regular(mtry(range = c(1,6)), # How many Variables to choose from
# researches have found log of total variables is enough
min_n(),
levels = 5)
## Set up grid of tuning values
tuning_grid <- grid_regular(mtry(range = c(1,9)), # How many Variables to choose from
# researches have found log of total variables is enough
min_n(),
levels = 5)
# Set up K-fold CV
folds <- vfold_cv(amazon_train, v = 5, repeats=1)
# Cross Validation
CV_results <- amazon_workflow %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(roc_auc))
library(doParallel)
num_cores <- parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
library(tidyverse)
library(embed) # for target encoding
library(vroom)
library(DataExplorer)
library(patchwork)
library(tidymodels)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
setwd('~/College/Stat348/AmazonEmployeeAccess')
# Load data
amazon_train <- vroom('./train.csv')
amazon_train$ACTION <- as.factor(amazon_train$ACTION)
amazon_test <- vroom('./test.csv')
# Recipe
my_recipe <- recipe(ACTION~., data=amazon_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
# step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
# step_normalize(all_numeric_predictors()) %>%
# step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
step_smote(all_outcomes(), neighbors=5)
# Model
rf_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=250) %>%
set_engine("ranger") %>%
set_mode("classification")
## Workflow
amazon_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod) %>%
fit(data = amazon_train)
## Set up grid of tuning values
tuning_grid <- grid_regular(mtry(range = c(1,9)), # How many Variables to choose from
# researches have found log of total variables is enough
min_n(),
levels = 5)
# Set up K-fold CV
folds <- vfold_cv(amazon_train, v = 5, repeats=1)
# Cross Validation
CV_results <- amazon_workflow %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(roc_auc))
# Find best tuning parameters
bestTune <- CV_results %>%
select_best("roc_auc")
# Finalize workflow
final_wf <- amazon_workflow %>%
finalize_workflow(bestTune) %>%
fit(data=amazon_train)
# Predict
amazon_predictions <- final_wf %>%
predict(new_data = amazon_test, type = "prob")
# Format table
amazon_test$Action <- amazon_predictions$.pred_1
results <- amazon_test %>%
rename(Id = id) %>%
select(Id, Action)
# get csv file
vroom_write(results, 'AmazonPredsrf.csv', delim = ",")
stopCluster(cl)
library(tidyverse)
library(vroom)
setwd('~/College/Stat348/GGG')
# Load data
ggg <- vroom('./trainWithMissingValues.csv')
skim(ggg)
dplyr::glimpse(ggg)
plot_missing(ggg)
library(tidymodels)
dplyr::glimpse(ggg)
# Set my receipe
my_recipe <- recipe(type~., data=ggg) %>%
step_impute_knn(var, impute_with = imp_vars(hair_length, rotting_flesh, bone_length), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
# Set my receipe
my_recipe <- recipe(type~., data=ggg) %>%
step_impute_knn(var, impute_with = imp_vars(hair_length, rotting_flesh, bone_length), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
train_clean <- bake(prep, new_data = ggg)
# Load data
ggg <- vroom('./trainWithMissingValues.csv')
# Set my receipe
my_recipe <- recipe(type~., data=ggg) %>%
step_impute_knn(var, impute_with = imp_vars(hair_length, rotting_flesh, bone_length), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
# Set my receipe
my_recipe <- recipe(type~., data=ggg) %>%
step_impute_knn(hair_length, impute_with = imp_vars(rotting_flesh, bone_length, type, colorl, has_soul), neighbors = 5) %>%
step_impute_knn(rotting_flesh, impute_with = imp_vars(hair_length, bone_length, type, colorl, has_soul), neighbors = 5) %>%
step_impute_knn(bone_length, impute_with = imp_vars(hair_length, type, colorl, has_soul), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
# Set my receipe
my_recipe <- recipe(type~., data=ggg) %>%
step_impute_knn(hair_length, impute_with = imp_vars(rotting_flesh, bone_length, type, color, has_soul), neighbors = 5) %>%
step_impute_knn(rotting_flesh, impute_with = imp_vars(hair_length, bone_length, type, color, has_soul), neighbors = 5) %>%
step_impute_knn(bone_length, impute_with = imp_vars(hair_length, type, color, has_soul), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
train_clean <- bake(prep, new_data = ggg)
# Set my receipe
my_recipe <- recipe(type~., data=ggg) %>%
step_impute_knn(hair_length, impute_with = imp_vars(rotting_flesh, bone_length, has_soul), neighbors = 5) %>%
step_impute_knn(rotting_flesh, impute_with = imp_vars(hair_length, bone_length, has_soul), neighbors = 5) %>%
step_impute_knn(bone_length, impute_with = imp_vars(hair_length, has_soul), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
train_clean <- bake(prep, new_data = ggg)
train_clean
# Calculate RMSE of the imputations
rmse_vec(ggg[is.na(missSet)], train_clean[is.na(missSet)])
trainSet <- vroom('./train.csv')
# Load data
missSet <- vroom('./trainWithMissingValues.csv')
# Set my receipe
my_recipe <- recipe(type~., data=missSet) %>%
step_impute_knn(hair_length, impute_with = imp_vars(rotting_flesh, bone_length, has_soul), neighbors = 5) %>%
step_impute_knn(rotting_flesh, impute_with = imp_vars(hair_length, bone_length, has_soul), neighbors = 5) %>%
step_impute_knn(bone_length, impute_with = imp_vars(hair_length, has_soul), neighbors = 5)
# apply the recipe to your data
prep <- prep(my_recipe)
# apply the recipe to your data
prep <- prep(my_recipe)
missing_clean <- bake(prep, new_data = ggg)
imputedSet <- bake(prep, new_data = ggg)
# Calculate RMSE of the imputations
rmse_vec(trainSet[is.na(missSet)], imputedSet[is.na(missSet)])
