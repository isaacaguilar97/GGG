metrics=metric_set(accuracy()))
## Set up K-fold CV
folds <- vfold_cv(amazon_train, v = 5, repeats=1)
dplyr::glimpse(ggg)
dplyr::glimpse(trainSet)
plot_missing(trainSet)
library(DataExplorer)
plot_missing(trainSet)
imputedSet <- bake(prep, new_data = trainSet)
plot_correlation(trainSet)
plot_histogram(trainSet)
GGally::ggpairs(trainSet)
# Set my recipe
my_recipe <- recipe(type~., data=trainSet)
## nb model
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid <- grid_regular(smoothness(),
Laplace(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(amazon_train, v = 5, repeats=1)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 5, repeats=1)
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(accuracy()))
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("accuracy")
## Finalize workflow
final_wf <- nb_wf %>%
finalize_workflow(bestTune) %>%
fit(data=amazon_train)
testSet <- vroom('./test.csv')
## Finalize workflow
final_wf <- nb_wf %>%
finalize_workflow(bestTune) %>%
fit(data=trainSet)
## Predict
amazon_predictions <- final_wf %>%
predict(new_data = testSet, type = "class")
## Predict
predictions <- final_wf %>%
predict(new_data = testSet, type = "class")
predictions
testSet
# Format table
testSet$type <- predictions$.pred_class
# Format table
testSet$type <- predictions$.pred_class
results <- testSett %>%
select(id, type)
results <- testSet %>%
select(id, type)
# get csv file
vroom_write(results, 'GGGPredsnb.csv', delim = ",")
# Set my recipe
my_recipe <- recipe(type~., data=trainSet) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=.9)
## nb model
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid <- grid_regular(smoothness(),
Laplace(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 5, repeats=1)
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("accuracy")
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(accuracy))
dplyr::glimpse(trainSet)
# Set my recipe
my_recipe <- recipe(type~., data=trainSet) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=.9)
## nb model
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid <- grid_regular(smoothness(),
Laplace(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 5, repeats=1)
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("accuracy")
## Finalize workflow
final_wf <- nb_wf %>%
finalize_workflow(bestTune) %>%
fit(data=trainSet)
## Predict
predictions <- final_wf %>%
predict(new_data = testSet, type = "class")
# Format table
testSet$type <- predictions$.pred_class
results <- testSet %>%
select(id, type)
# get csv file
vroom_write(results, 'GGGPredsnb.csv', delim = ",")
nn_recipe <- recipe(type~., data=trainSet) %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
library(doParallel)
num_cores <- parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
library(tidyverse)
library(vroom)
library(tidymodels)
library(naivebayes)
library(discrim)
library(DataExplorer)
# Load data
missSet <- vroom('./trainWithMissingValues.csv')
trainSet <- vroom('./train.csv')
testSet <- vroom('./test.csv')
nn_recipe <- recipe(type~., data=trainSet) %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=5)
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_nn_tuneGrid,
metrics=metric_set(accuracy))
nb_nn <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=5)
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_nn_tuneGrid,
metrics=metric_set(accuracy))
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=5)
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_model <- mlp(hidden_units = tune(),
epochs = 50, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=5)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune <- tuned_nn  %>%
select_best("accuracy")
# Finalize Workflow
final_wf <- nn_wf %>%
finalize_workflow(bestTune) %>%
fit(data=trainSet)
## Predict
predictions <- final_wf %>%
predict(new_data = testSet, type = "class")
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
stopCluster(cl)
# Format table
testSet$type <- predictions$.pred_class
results <- testSet %>%
select(id, type)
# get csv file
vroom_write(results, 'GGGPredsnb.csv', delim = ",")
# get csv file
vroom_write(results, 'GGGPredsnn.csv', delim = ",")
# Plot graph
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
nn_recipe <- recipe(type~., data=trainSet) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_recipe <- recipe(type~., data=trainSet) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
library(tidyverse)
library(vroom)
library(tidymodels)
library(naivebayes)
library(discrim)
library(DataExplorer)
# Load data
missSet <- vroom('./trainWithMissingValues.csv')
trainSet <- vroom('./train.csv')
testSet <- vroom('./test.csv')
nn_recipe <- recipe(type~., data=trainSet) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("mlp") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
load("./MyFile.RData")
load("./MyFile.RData")
nn_recipe <- recipe(type~., data=trainSet) %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet", verbose=0) %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_recipe <- recipe(type~., data=trainSet) %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet", verbose=0) %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_recipe
nn_model
nn_wf
nn_tuneGrid
tuned_nn
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
setwd('~/College/Stat348/GGG')
# Load data
missSet <- vroom('./trainWithMissingValues.csv')
trainSet <- vroom('./train.csv')
testSet <- vroom('./test.csv')
nn_recipe <- recipe(type~., data=trainSet) %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_recipe <- recipe(type~., data=trainSet) %>%
update_role(id, new_role="id") %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(grid=nn_tuneGrid,
metrics=metric_set(accuracy))
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(grid=nn_tuneGrid,
metrics=metric_set(accuracy))
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
nn_recipe <- recipe(type~., data=trainSet) %>%
update_role(id, new_role="id") %>%
step_dummy(all_nominal_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_model <- mlp(hidden_units = tune(),
epochs = 50 #or 100 or 250
) %>%
set_engine("nnet") %>% #verbose = 0 prints off less (or nnet)
set_mode("classification")
# Workflow
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
# Tune
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 100)),
levels=10)
## Set up K-fold CV
folds <- vfold_cv(trainSet, v = 10, repeats=1)
# Cross Validation
tuned_nn <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
head_train
head(trainSet)
